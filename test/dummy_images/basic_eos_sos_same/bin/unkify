#!/usr/bin/env python
"""
Unkify script defined based on a language model's spec and tokenization output.
"""

import json
import shlex
import subprocess
import sys


input_file = sys.argv[1]


# Retrieve unk types from spec JSON.
spec = json.loads(subprocess.check_output("spec"))
unk_types = spec["vocabulary"]["unk_types"]

# Run `tokenize`, and compute mask using unk types list.
tokenized_lines = subprocess.check_output(["tokenize", shlex.quote(input_file)]) \
        .decode("utf-8").splitlines()

for tokenized_line in tokenized_lines:
    unked_line = ["1" if token in unk_types else "0"
                  for token in tokenized_line.strip().split(" ")]
    print(" ".join(unked_line))
